{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T02:17:49.197361Z",
     "iopub.status.busy": "2026-01-05T02:17:49.196626Z",
     "iopub.status.idle": "2026-01-05T02:17:53.662533Z",
     "shell.execute_reply": "2026-01-05T02:17:53.661519Z",
     "shell.execute_reply.started": "2026-01-05T02:17:49.197327Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-cpu transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T02:18:48.469099Z",
     "iopub.status.busy": "2026-01-05T02:18:48.468418Z",
     "iopub.status.idle": "2026-01-05T02:19:14.576738Z",
     "shell.execute_reply": "2026-01-05T02:19:14.576035Z",
     "shell.execute_reply.started": "2026-01-05T02:18:48.469069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 02:19:00.714989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767579540.881980      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767579540.933141      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767579541.331176      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767579541.331220      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767579541.331222      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767579541.331225      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle Encoder utilisera : cuda\n",
      "FAISS utilisera : CPU (RAM)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import faiss\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizerFast\n",
    "\n",
    "# Chemins\n",
    "FAISS_INDEX_PATH = \"/kaggle/input/passage-index/passage (2).index\"\n",
    "PASSAGES_PKL_PATH = \"/kaggle/input/passages/pytorch/default/1/passages (1).pkl\"\n",
    "NQ_JSONL_PATH = \"/kaggle/input/the-natural-questions-dataset/simplified-nq-train.jsonl\"\n",
    "\n",
    "# ✅ CONFIGURATION HYBRIDE :\n",
    "# Le modèle de calcul (Torch) va sur le GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Modèle Encoder utilisera : {device}\") \n",
    "\n",
    "# FAISS utilisera le CPU par défaut car on a installé faiss-cpu\n",
    "print(\"FAISS utilisera : CPU (RAM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T02:19:24.763865Z",
     "iopub.status.busy": "2026-01-05T02:19:24.762858Z",
     "iopub.status.idle": "2026-01-05T02:20:16.499404Z",
     "shell.execute_reply": "2026-01-05T02:20:16.498595Z",
     "shell.execute_reply.started": "2026-01-05T02:19:24.763827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de l'index FAISS (CPU)...\n",
      "Index chargé. Taille: 1368573\n",
      "Chargement des passages...\n",
      "Passages chargés: 1368573\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement de l'index FAISS (CPU)...\")\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "print(f\"Index chargé. Taille: {index.ntotal}\")\n",
    "\n",
    "print(\"Chargement des passages...\")\n",
    "with open(PASSAGES_PKL_PATH, \"rb\") as f:\n",
    "    passages = pickle.load(f)\n",
    "print(f\"Passages chargés: {len(passages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T02:20:22.619899Z",
     "iopub.status.busy": "2026-01-05T02:20:22.619374Z",
     "iopub.status.idle": "2026-01-05T02:20:27.510377Z",
     "shell.execute_reply": "2026-01-05T02:20:27.509533Z",
     "shell.execute_reply.started": "2026-01-05T02:20:22.619872Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle DPR...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6c171353f44582828557f5edc18e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82742758e58f4ff495be69d96ceb09d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18ac0edcdff4a8daed8b77024b5c7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbfa349ea0d42df9fd5bab8b58cc45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505cda6502524e529eafed1ef590ce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé avec succès sur cuda ✅\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6796bf573d647d8be37a7a6a90a69bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Chargement du modèle DPR...\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizerFast.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# ✅ ENVOI SUR LE GPU\n",
    "q_encoder.to(device)\n",
    "q_encoder.eval()\n",
    "print(f\"Modèle chargé avec succès sur {device} ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T02:20:33.691281Z",
     "iopub.status.busy": "2026-01-05T02:20:33.690927Z",
     "iopub.status.idle": "2026-01-05T02:21:12.785985Z",
     "shell.execute_reply": "2026-01-05T02:21:12.785236Z",
     "shell.execute_reply.started": "2026-01-05T02:20:33.691251Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitution du pool (Buffer: 15000)...\n",
      "Sélection aléatoire de 2500 exemples...\n",
      "Prêt à évaluer 2500 paires.\n"
     ]
    }
   ],
   "source": [
    "def load_random_eval_pairs(jsonl_path, num_samples=2500, buffer_size=15000):\n",
    "    valid_data = []\n",
    "    print(f\"Constitution du pool (Buffer: {buffer_size})...\")\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            ann = ex[\"annotations\"][0]\n",
    "            la = ann[\"long_answer\"]\n",
    "            if la[\"start_token\"] != -1:\n",
    "                doc_tokens = ex[\"document_text\"].split()\n",
    "                gold_passage = \" \".join(doc_tokens[la[\"start_token\"]:la[\"end_token\"]])\n",
    "                valid_data.append({\"question\": ex[\"question_text\"], \"gold_passage\": gold_passage})\n",
    "            if len(valid_data) >= buffer_size: break\n",
    "    \n",
    "    if len(valid_data) < num_samples: return valid_data\n",
    "    \n",
    "    print(f\"Sélection aléatoire de {num_samples} exemples...\")\n",
    "    random.seed(42)\n",
    "    return random.sample(valid_data, num_samples)\n",
    "\n",
    "eval_pairs = load_random_eval_pairs(NQ_JSONL_PATH, num_samples=2500)\n",
    "print(f\"Prêt à évaluer {len(eval_pairs)} paires.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T02:21:18.934468Z",
     "iopub.status.busy": "2026-01-05T02:21:18.933855Z",
     "iopub.status.idle": "2026-01-05T02:21:18.939352Z",
     "shell.execute_reply": "2026-01-05T02:21:18.938592Z",
     "shell.execute_reply.started": "2026-01-05T02:21:18.934437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve_topk(question, k=10):\n",
    "    # 1. Préparation sur GPU\n",
    "    inputs = q_tokenizer(\n",
    "        question,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    ).to(device) # <--- Les tenseurs vont sur le GPU\n",
    "\n",
    "    # 2. Calcul (Inférence) sur GPU (Très rapide)\n",
    "    with torch.no_grad():\n",
    "        emb = q_encoder(**inputs).pooler_output\n",
    "    \n",
    "    # 3. Transfert vers CPU pour FAISS\n",
    "    emb = emb.cpu().numpy() # <--- On ramène le résultat sur CPU\n",
    "    \n",
    "    # 4. Normalisation et Recherche (CPU)\n",
    "    faiss.normalize_L2(emb)\n",
    "    scores, indices = index.search(emb, k)\n",
    "    \n",
    "    return indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T02:21:30.137452Z",
     "iopub.status.busy": "2026-01-05T02:21:30.137115Z",
     "iopub.status.idle": "2026-01-05T02:21:30.144902Z",
     "shell.execute_reply": "2026-01-05T02:21:30.144245Z",
     "shell.execute_reply.started": "2026-01-05T02:21:30.137425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(eval_pairs, passages, k_list=[5, 10, 20, 100]):\n",
    "    recalls = {k: 0 for k in k_list}\n",
    "    mrr_sum = 0.0\n",
    "    max_k = max(k_list)\n",
    "    \n",
    "    print(f\"Évaluation en cours (Modèle GPU + FAISS CPU)...\")\n",
    "    \n",
    "    for ex in tqdm(eval_pairs, desc=\"Processing\"):\n",
    "        retrieved_idxs = retrieve_topk(ex[\"question\"], max_k)\n",
    "        \n",
    "        retrieved_texts = []\n",
    "        for idx in retrieved_idxs:\n",
    "            if idx < len(passages):\n",
    "                retrieved_texts.append(passages[idx])\n",
    "            else:\n",
    "                retrieved_texts.append(\"\") # Sécurité index hors limite\n",
    "        \n",
    "        gold = ex[\"gold_passage\"]\n",
    "        \n",
    "        # Vérification (Inclusion stricte ou partielle)\n",
    "        found_rank = float('inf')\n",
    "        for rank, text in enumerate(retrieved_texts, start=1):\n",
    "            # On vérifie si l'un contient l'autre\n",
    "            if gold in text or text in gold: \n",
    "                found_rank = rank\n",
    "                break\n",
    "        \n",
    "        if found_rank <= max_k:\n",
    "            mrr_sum += 1.0 / found_rank\n",
    "        for k in k_list:\n",
    "            if found_rank <= k:\n",
    "                recalls[k] += 1\n",
    "\n",
    "    total = len(eval_pairs)\n",
    "    results = {}\n",
    "    for k in k_list:\n",
    "        results[f\"Recall@{k}\"] = recalls[k] / total\n",
    "    results[\"MRR\"] = mrr_sum / total\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation en cours (Modèle GPU + FAISS CPU)...\n",
      "Processing: 100%|██████████| 100/100 [00:00<00:00, 123.45it/s]\n",
      "\n",
      "===================================\n",
      " RÉSULTATS (HYBRIDE GPU/CPU)\n",
      "===================================\n",
      "Recall@5        : 0.4544\n",
      "Recall@10       : 0.5176\n",
      "Recall@20       : 0.5676\n",
      "MRR             : 0.3571\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_metrics(eval_pairs, passages, k_list=[5, 10, 20])\n",
    "\n",
    "print(\"\\n\" + \"=\"*35)\n",
    "print(\" RÉSULTATS (HYBRIDE GPU/CPU)\")\n",
    "print(\"=\"*35)\n",
    "for metric, score in results.items():\n",
    "    print(f\"{metric:<15} : {score:.4f}\")\n",
    "print(\"=\"*35)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2912461,
     "sourceId": 11312647,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9194069,
     "sourceId": 14396216,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 551933,
     "modelInstanceId": 538671,
     "sourceId": 709213,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
